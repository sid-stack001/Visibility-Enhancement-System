# Visibility Enhancement System

Real-Time Dehazing & Deraining using CycleGAN

![WhatsApp Image 2025-08-16 at 14 50 31_e451d44f](https://github.com/user-attachments/assets/3111af34-0818-4a73-8a8d-6bc296bd94f7)

## ğŸ“Œ Overview

Weather conditions like **haze** and **rain** degrade visibility, reducing the reliability of computer vision systems used in:

- ğŸš— **Autonomous navigation**
- ğŸ“¹ **Surveillance**
- ğŸ›°ï¸ **Remote sensing**
- ğŸ¥ **Image & video enhancement**

This project introduces a **CycleGAN-based system** for **high-resolution multi-weather degradation removal**. Unlike conventional dehazing/deraining approaches, our model:

- Operates on **native image resolution** (avoiding downscaling)
- Handles **both haze & rain simultaneously**
- Achieves **competitive state-of-the-art results** on benchmark datasets

---

## âœ¨ Key Contributions

- ğŸ”¹ **Custom Dataset**: 7,614 unpaired images (synthetic + real) from REVIDE, Rain1400, and Dense-Haze
- ğŸ”¹ **Architecture Modifications**: CycleGAN tailored for **high-fidelity restoration**
- ğŸ”¹ **Training Optimizations**: ADAM optimizer, 100 epochs, unpaired image learning
- ğŸ”¹ **Benchmark Testing**: Evaluated on **SOTS** and **O-HAZE** datasets
- ğŸ”¹ **Results**: Achieved **PSNR ~29 dB** and competitive **SSIM scores**

---

## ğŸ§  Methodology

ğŸ”¹ Why CycleGAN?

CycleGAN enables image-to-image translation without paired datasets, making it ideal for visibility enhancement where paired clean-weather images are hard to obtain.

ğŸ”¹ Architecture

Generator: Translates hazy/rainy frames into clear frames

Discriminator: Ensures generated outputs are visually realistic

Cycle Consistency Loss: Maintains structural fidelity between degraded and restored images.
<img width="692" height="531" alt="image" src="https://github.com/user-attachments/assets/460a0ba3-60d6-4e07-a263-08a106a9c352" />

## ğŸ“‚ Project Structure  

```bash
â”œâ”€â”€ canny_edge_det/      # Checks image features through canny edges
â”œâ”€â”€ checkpoints/         # Saved model checkpoints  
â”œâ”€â”€ data/                # Datasets (train/test splits)  
â”œâ”€â”€ datasets/            # Dataset processing scripts  
â”œâ”€â”€ docs/                # Documentation and related resources  
â”œâ”€â”€ imgs/                # Images for README/demo  
â”œâ”€â”€ input_images/        # Input images for testing  
â”œâ”€â”€ input_videos/        # Input videos for testing
â”œâ”€â”€ models/              # Stores models
â”œâ”€â”€ onnx/                # Converts model to .onnx format
â”œâ”€â”€ options/             # Configuration files and options  
â”œâ”€â”€ results/             # Generated results (images/videos)  
â”œâ”€â”€ results_past/        # Previous experiment results  
â”œâ”€â”€ scripts/             # Utility scripts for training/inference  
â”œâ”€â”€ util/                # Helper functions and utilities  
â”‚
â”œâ”€â”€ LICENSE              # License file (MIT)  
â”œâ”€â”€ README.md            # Project documentation  
â”œâ”€â”€ desktop.ini          # Windows system file (can be ignored)  
â”œâ”€â”€ server.py            # API/Server script  
â”œâ”€â”€ test.py              # Testing script  
â”œâ”€â”€ test_native_live.py  # Live webcam inference (native resolution)  
â”œâ”€â”€ test_pic.py          # Single image test script  
â”œâ”€â”€ test_resize.py       # Test with resized inputs  
â”œâ”€â”€ test_server.py       # Server-based test script  
â”œâ”€â”€ test_video.py        # Video file inference  
â”œâ”€â”€ test_video_live.py   # Live video stream inference  
â”œâ”€â”€ train.py             # Training entry point  
â”œâ”€â”€ yolov8.pt            # YOLOv8 weights (for auxiliary tasks if used)  

```
## ğŸ“‚ Dataset

| Dataset    | Train Images | Test Images | Total    |
| ---------- | ------------ | ----------- | -------- |
| REVIDE     | 3310         | 194         | 3504     |
| Rain1400   | 3600         | 400         | 4000     |
| Dense Haze | 80           | 30          | 110      |
| **Total**  | **6990**     | **624**     | **7614** |

---


## ğŸ Getting Started  

Follow these steps to quickly run the Visibility Enhancement System on your machine.  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/your-username/visibility-enhancement.git
cd visibility-enhancement
```

### 2ï¸âƒ£ Setup Environment

Itâ€™s recommended to use Conda or virtualenv.
```
conda create -n vis-enhance python=3.9
conda activate vis-enhance
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run Inference

On a single image:
```
python test_pic.py --input input_images/sample.jpg --output results/output.jpg
```
On a video file:
```
python test_video.py --input input_videos/sample.mp4 --output results/output.mp4
```
Live webcam demo (native resolution):
```
python test_native_live.py

```
---

## ğŸ“Š Results

<img width="975" height="348" alt="image" src="https://github.com/user-attachments/assets/c0864a59-5498-410d-9b49-98455d7d2b66" />
<img width="975" height="296" alt="image" src="https://github.com/user-attachments/assets/7055dfe5-16ae-428e-a5dc-20b0d8089890" />
<img width="975" height="313" alt="image" src="https://github.com/user-attachments/assets/d87771c2-973d-4079-8370-612d4f8eff86" />

## ğŸ“ˆ Evaluation Metrics

- PSNR (Peak Signal-to-Noise Ratio) - 28.09
- SSIM (Structural Similarity Index) - 0.78

Tested on the SOTS indoor dataset.

## ğŸ› ï¸ Tech Stack

ğŸ Python 3.9+

ğŸ”¥ PyTorch

ğŸ¥ OpenCV

âš¡ CUDA/cuDNN

## ğŸ“¢ Future Work

ğŸŒŒ Extending to fog, snow, and low-light scenarios

ğŸ“± Developing a lightweight version for mobile & edge devices

ğŸ¤– ROS integration for autonomous systems

ğŸ§© Exploring transformers and diffusion-based approaches

## ğŸ™ Acknowledgements  

This work is built upon the excellent open-source implementation of CycleGAN and Pix2Pix provided by [junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).  
We sincerely thank the authors for their contributions to the research community. 
